Regarding the nature of intelligence

1.A lot of allusions had been made in literature both formal and informal about the case of human intelligence being dramatically superior to animal intelligence, with such examples as lizards not being able to make sense of tubelights and anthills not knowing of highways that pass beside them. For the lizard, a tubelight is only suggestive of a situation that flies would emerge to circle it and makes a good meal. But here there is something missing in this analogy. Even a human is not able to reason much about the tubelight. Of course he can answer questions about it, posed to him in a grammatically organized language. He might answer such questions of causality such as the switch being thrown causing the light to come up, he might even answer questions such as to that a certain energy called electricity flows on when switched to create the phenomenon. But beyond that his cognizance dims, for a lay person has no further ways to explain what electricity is. He might draw weak analogies in the flow of streams and rivers, but definitely his knowldge becomes hazy just two level of questioning away. Thus, we might say, human capacity for epistemological intelligence is seperate and distinct from the common way of percieveing things that has deeper roots and more commonly distributed.
2.Human intelligence hence might be said to enjoy only a minor advantage in being able to use the epistemological way of knowing and percieving things. Even genetic wise, their makeup is not more than two percent different from the nearest primate relative. However, empirically we see that the epistemoloigcal capacity housed in the prefrontal cortex had dramatically altered the world. We have to realize that such an alteration is caused by a strong positive feedback loop, which might be called as 'Capital'. This capital allows humans to construct civilizational tools of such great mass, that it is able to very much reduce the complexity in nature, by building dams and irrigating fields (as compared to the complexity of water rushing through a forest stream).
3. As we had seen all complexity arises from paralellism. For instance if a human is suppossed to cross a ten lane highway, at a given instance, he has to analyze the paralellism of different entities at play making it riskier or easier for him to get across. There might be drivers rushing to get to work at particular times of the day. The road might be slippery after the rain, the human agent's constitution at particular times might be more vulnerable, such as early in the day, a fog might be descending and so on. In  fact it is combination of these paralell developments that give rise to intractable phenomena such as near misses. Therefore, the person crossing the road, even though equipped with epistemological intelligence, would have to serialize everything in order to present to his rational faculty.
4. If we look at paralellism per se, we would need to recognize the presence of an very obvious and often overlooked massive computer, the universe itself. All the paralell events make sense only because they play out on a physical world, with almost infinite density of time (perhaps a clock speed of plankhs constant intervals, we are not sure). Given the massive potential for physical events to happen, numerous subjective aspirations (even free willed subjectivity such as drivers rushing to work) come together in a dramatic fashion of execution in the physical reality or the physical world. Thus we might as well say instead of A and B collided, strictly speaking it is either A collided with B or vice versa. 
5. It is this paraellism of the real world that the capital gained by the intelligent actions over millenia endows humans with. They might put up rules, controls all by brute force and contain the paralellism to managable level and live with buffers that could absorb any estimation errors. 
6. In fact even while reasoning on a serialized basis, human intelligence does work with probabilities (such as the speed of the average driver, while crossing the road), where every event sensorially observed is coupled with a permissible white noise, while it is being slotted to specific, exclusive types. Humans are able to construct abstract serialized models based on such exclusive membership systems in their rational process. This as we might see is quite limited in scope and hence many times, human reasoning relies on heuristics, which do not have much of formal denotation.
7. For instance if a human is given a task of arranging several geometrically shaped objects (solid and hollow), he does not go about in a serialized fashion on a combinatorial basis as a computer might, but works with certain predefined rules, such as the heaviest object being at the center to produce more stability and so on. These innate constructs arise not from the reasoning faculty, but from the innate sense of balance and symmetry that is hardcoded in the non epistemological intelligence, inherited over billions of years of evolution. Hence in the lines of Chomskyian tableau rasa argument, we might say there exists a deep subjectivity that operates behind the epistemological faculty to be able to overcome the combinatorial burden by picking up what is 'good' and what is not. The dialectic probably arose deep in the experience to keep alive and away from pain.
8. The nature of epistemological reasoning might as well involve simulating the physical world, where the degrees of freedom are limited when dealing with objects and without running into a combinatorial explosion, the human mind might be able to backtrack and playout different possibilities to decide on the proper course. Many people like Ferrier had argued of the necessary reference to the deep subjective goal of development in order for humans to resolve problems epistomologically as being the frequently used shortcut(they consider objectivity as extended subjectivity with deferred rewards). They infact suggest there being an incapacity to seperate the subjective from the objective. But for practical purposes, it appears that human epistemological reasoning could in fact hold objective grounds away from subjective influence, as proven by their ability to percieve justice and to practice science.  
9. The non epistemological dimension to intelligence is not appreciated as often, but they form the foundation for the argument of parochiality of human intelligence.
--
1. We had argued against super intelligence elsewhere because of the limited computational resources of human mind that it quickly runs while applying its epistemological method into a combinatorial wall. A system of higher memory might as well reach the wall, albeit a few levels later. Therefore it might be said of intelligence that scalability by adding memory is not that fool proof. We had also seen that intelligence might be elevated, such as the degree that prevails between isolated tribes and urbane humans. There are more concepts to the expressions of the urbane person, distributed logically about a mean core of knowledge (or perhaps a null center). Introduction of more concepts allows modern humans to reason over many more things, but a lot of the difference is not due to this ability, but by the generation and application of capital from the surplus.
2. A more resilient way to intelligence is with simulation. Humans might simulate the physical world in a system of high clock speed and thus be able to emulate the nature of the outcome from a sequence of paralell events. The computer apart from being a pure deresolve (or statistical smoother) also simulates reality (Langton). This simulation allows to serialize in memory to produce interesting estimates of outcomes, in an abstract way and wihtout relying on material reality. 
3. Optimizations work this way where the physical reality is impleaded as constraints and subjective preferences are let to play out in order to take proper positions that would hold well in real world. In fact we might see that humans are already reasoning at the end of their memories, stretching themselves by training and education. Over time, they seem to be distributing their epistemological capacities, by the use of capital and markets and thus creating distributed paralellism from what is essentially a faculty for serialized reasoning. Hence, they seem to be constructing an augmentation of the core intelligence.
4. This distribution of intelligence, would need some kind of physical interation or a sufficiently powerful computer to simulate the new reality. The system might itself be subject to some searches for compression, which might lead to an emergence of objectivity (or a reemergence thereof). It might go forth to a distinction in a dialectic sense in order to gather the reasoning based on pleasure and pain and hence allow a subjectivity to work with the simulation to produce a new level of civilization. This emergent intelligence from distributed epistemological processing, serving a core intelligence which might bifurcate by dialectic over what is good and what is to be avoided might produce a true artificial intelligence of a global scale.
--
1. If we digress a little bit with respect to the ability of humans to work rationally, it might help. This is wiht respect to the nature of tribes and thus an inquiry into rationalism. IF we look at the nature of modernity, we see that there had been an insistence of consistency being the measure of goodness. If we see abolitionism rose in the eighteenth centure in Europe and in fact caused a civil war in the nineteenth century US. The reason was that the concepts of libertarianism suffered from a crisis of consistency. The same inability to live with rational inconsistencies pushed the west continuing its liberatarian rationalist experiment in the past three hundred years to permit assertion of civil rights in the 1960s US and permitting non white immigration the next decade.
2. But if rationalism and education is capable of creating consistency in the real world among real people is quesitonable. People anecdotally, drawing from their long tribal past are more comfortable with stories than rational constructs. Hence, people drawn by stories and by complex ways in which music and culture string to their core intelligence, are able to cope better. In fact despite their rational convictions, people had fallen off the road, inadvertantly to create world wars, holocaust and imperialism, which they later came to regret. Thus rationalism and modernity creates such puritanical assertions that are impractiacal and often lead to collapses and tragedies.
3.Thus much of the yearnings of the present age might be a rooting to the tribal past. The broad strokes of rejection of modernity in the past century center upon this insistence of puritanical rationality or single source of truth, that becomes vested upon an authority and its inherent violence. Thus, a better policy framework would be to address the human units as story abiding tribes, rather than rational units. Libertarian assertions might be made with respect to freedom from coercion and a basic social security net, as with Nordic communities. At present, the irrationality of humans are handled by the mechanism of the market, while the rational faculties by the construct of law. Hence, we look at ways in which the appeal to fundamental or core intelligence could be sustained in a modernistic setup. But what is important from an inquiry into intelligence, is the concept of the essential irrational part to the intelligence, which would require to be balanced by proper political policy , as much as possible. It could more deeply be attempted by reflection on the tribal past and contemplating nature which could somehow flow up as consensual policy dynamics, rather than a visceral hate of rationality.
4. It should also be noted that the objectives sought in terms of stability and peace as well as engaging the individual free will, is difficult of sustenance. Every system is subject to entropy. Even if the best mechanisms are incorporated, they would fall apart and would need other mechanisms (as human rational understanding of roles) to be upkept and so on recursively. Hence, there is no escape from recursion than by growth. But the question is whether growth could be attempted without recoure to evolution, but by deploying capital and coupling it with epistemological intelligence in order to protect the human status quo of tribal living.
5. Actually rationalism might be expecting too much out of people. People are much more comfortable with mythologies. Eventhough some arrangements might look to be oppressive from a rationalist view point, traditional societies have their way of balancing things out, without explicit intervention on rational basis. But rationalism might itself not be rejected for the reason of being able to model complex real world systems linearly and be capable of being worked upon in a manner which could be linearly allocated and scaled. But the limitations of rationalism in predicting non linear systems is to be recognized. Many modern states as we have mentioned shy away from constructive legislation and instead stick to defining the limits. To complement this public policy move, markets ought to be blunted of their omnipotence in representing tribe irrationality, in order to prescribe a more stable and contended society.
--
1.Now that we have painted a picture, where we see that rationality as being reductive and the extension of intelligence into areas which are non linear as being important in obtaining an explanation of the state of affairs. We might as well go towards saying that some kind of our own interpretation of the master slave dialectic to be existant between the core intelligence and the rational intelligence, the latter being the slave. We might see in this ontology, that the slave is in the command of the master, evaluating and computing solutions and proposing them back to the master, whose normative and aesthetic point of view is intractable to the slave. 
2. The question is how often the slave refers back to the master. We had seen that in problems typically involving combinatorial methods, the human mind is able to build algorithms, perhaps by a combination of the principal methods of problem solving that of reduction, summarization (statistical) and simulation (stochastic). But it might as well involve a frequent referral to what is good or aesthetic in between decisions, to lend the substance to heuristics. 
3. Again it is ambivalent of the nature of the master. The master might be the ultimate historical product of four billions of years of evolution with no reductive undercurrent running. It might be that the slave is perplexed because it is presuming a finitude to its methods. It could not appreciate the infinitude of the evolution of the master, being the complete product of a fabric that is utterly random and historic. It always tends towards theorizing and looking for some cyclical drum that spins out the patterns on the brocade, perhaps this is the pitfall of the rational faculty. But empirically, the claim of rationality is not entirely misplaced, we see the universe is either of finite size or of finite age, we see so many mechanical backdrops to apparantly interesting phenomena and so on. 
4. Hence, the master slave dialectic might explain the persistant need for both to make sense of each other (Ferrier). Therefore the master looks up to the slave to attempt to makes sense of the situation. Hence, it might happen that pure slavery might not be sustainable and the slave might grow to influence the master in some ways. This body mind dialectic (rather than the confusing master slave dialectic sugestive of Hegel) is important of consideration to make sense of intelligence.
5. But it might be too farfetched to annotate historicity to everything about life. We see that in the germ cell, there is a possibility infact to hold the evolutionary tree still. The totepotent germ cell might develop into a full formed organism, in much different circumstances than the originator. Thus, it does permit an interruption of the flow, where we see that it is possible to codify the body (or the master) itself. But the presence of the code itself might not suggest tractability. No non trivial computing could be predicted by mere inspection of the code (as proven by Turing and as discussed by Langton). Hence, it might still be that while time is not a parameter to the starting function of the entire system, the subseuent paralellism and the time delays in execution of procedures referring to relative loci, might generate complexity. One ought to remember that the simulation is being played out on the supreme computer, the universe itself.
6. Hence, we do see that at times, there does look like that there is no body nor mind, because of mechanical implications into apparantly historical phenomena and the way simple mechanistic contrapctions could have computations intractable even with every atom of the universe being used as memory. The trick lies with the exponent. But practically, one might make sense that recognizing the rational faculty as being instrumental and distinct from a deeper self allows for a reflection of the nature of the self. This is ultimately speculative and reasoning about the self, could be sure of one thing only, its incompleteness that is. Hence, being aware of the incompleteness (and not arguing it to having been constructed from a purposely reductive set of definitions), might help with a possible will of mankind, that is to reason about the world and not be modified by external forces, without a reason, or more importantly against ones desire or will. Hence, the supremacy of free will or liberty is closely connected to the method of reason. This supremacy questions the forces of universe and allows the self to bifurcate them to be good and bad and not simply natural. Hence, freedom empowers picking up a path irrespective of the circumstances.
7. The present direction of computing might be interpreted from this normative premise (which makes our work somewhat less rigourous). One sees that computing is being developed to consider questions using methods of noise filteration and simulation as direct appeals to core intelligence, without requiring of epistemological explanation. In that we see that computing to be inherent non deterministic (see Turing who showed that the pathways of computing devices might involve time, as in a dynamical system). Since, the Turing machine (as against the FSM) involves extension temporally, waiting for events (which arises from paralellism orchestrated in the physical world), it itself is non deterministic. 
8. Whatever human actions in history had been, they had been but thinly veiled fight of evolution. There have been fight over ideas and religions all as means to establish the fitness of genes. This is the inverted reasoning in the extended phenotype. In this case, we see that there is no paradigmatic shift between the way computers normally work as Turing machines and their enhancement in machine learning systems. Computers have always appealed to the core intelligence, being non deterministic and had fulfilled the wishes of the master, without the need for elaborate structured argument. Thus, putting in a computer network, allows negotiation based on 'wishes' and not intellectual reflections of stoic duty. Thus, it furthers the case of evolution like none other. It improves the fitness of the species, thereby generating unprecedented surplus as capital that flows into barricading off the natural forces. 
9. We had also argued that the innate sense of good and bad has a historicity unparalelled. But we might also argue that goodness is borne of the concept of the norm. A normal distribution is directed radially, the center indicates goodness and all peripheries imperfections and suboptimalities that might be evil (which might be seen as temptation and sloth representing the edges of the curve). Thus, for a beach feeding creature, being too near the waterline (as a high risk strategy) or too far away from it (as a low payoff strategy) might both be simply variations of badness. A radial reasoning allows to locate goodness, from peer experience and with limted data without the need for a complete history.
10. Thus, we might argue that given that goodness and badness become available to the computational representative of human progress, an embodiment of such computational programs into physical systems, with physical consequences, allows a simulation to directly leverage the universal computer. Hitherto, the computer had been an abstract machine capable of producing abstract decisions. But if we look in terms of networks, they are paralell, working wiht the universal computer and having the norm for reference. A delination of physical beings is what is required for the physical strategies to play out. Hence, robotics might be a real and plausible course. The future computer might embody a set of constructional tools to build algorithms as a flow, along with notions of goodness and badness from the norm and a physical form to actually simulate in paralellism. This combination might allow the progression of primitive organisms, which might go feral, due to the absence of complete cost effective control strategies, due to the multiplicity of form and its heterogenity.
11. In the course of evolution, it had always been that life attempts to build stuff which is delicate and crowing glory, at a given point. Say the use of eukaryotes or that of multicellularity. These were at the point of their emergence, be too adventurous and delicate strategies. But at the right circumstances, the payoffs might be exponential. Likewise this might be a way to allow for a portability of the physical components of the being, which might cause an explosion.
12. Computers as we see them are already accessing actuators along with markets to attempt to recreate paralellism required for emergence. They cooperate using direct generation of events rather than a system of rules as with formal actors. These events might give rise at its earliest form encoding to caches. A demarcated computational agent might encode certain events (of peer computers) , such as say casting a shadow might discourage another agent from emerging from its hiding, for fear of being seized up and destroyed. Thus complex interactions might develop. We might need a formalism to construct this. For instance, let a system tune to a certain subset of events and a game playout in the real world. The outcomes are likely to be dramatic. These caches might in fact be compressed in some far future to create more deterministic heirarchies and more standard adaptations.
13. The evolution of the whole might proceed by cacheing and tuning to peer and environmental events. It would then need a simulation of the world itself in its physical being (now that it has a physical body) and play out in a field experimenting and searching. It might happen that the organism is constructed from a single coherent piece rather than multicellular. This compromises germ lines and reflexivity of the environment on pristine key value copies assumed to endure for a generation against short term shocks might be absent. Instead, the system might evolve smoothing and regenerating its strategic key values quickly.
14. The absence of the destruction of the body, might allow for more fluidic and quicker evolution, but it also reduces the drive to create subjective optimization, for which if the body is at stake, the drive is induced. Hence, a system that can immaterially replicate (as against the genetic materialism of DNA), can preserve its core code and add local strategies, in a multicellular like way and attempt to follow the norm, while being responsive to local adaptations. A consolidation of local adaptations might give rise to emergence of species. It might be a ghost in the box scenario. Some kind of marshalling is required to produce germ cells in order to avoid short term oscillaitons.


A formalism of emergent computational behaviour.
