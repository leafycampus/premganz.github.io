Working with the do-calculus

A) A complexity model of computing 
From there we attempt to build upon a model of computing, where we can formalize the system with respect to its normative system.
1) Turing Model of computing.
Our idea would be to see the way in which computers are organized with respect to the system that they are created in, or embedded in. The system utilizing the computer, mostly uses it to simulate a machine or a market or a deliberative individual. That is to say if a system of business or any goal directed, normative organization involves the computer amidst it in order that it satisfies the missing link in the organization of individuals. An individual with a deliberative sense, a market or an organization needs to decide, between nontrivial choices at any point of time. These decisions are expected to be made by consensus (or if an individual, conscientous) if it is consistent and bias free. This is achieved by the development of a system which reduces the environment into a model, where problems could be posed to the system and decisions be obtained, in a simulated mode, in the interests of the stability of the system as a whole, at lower cost than negotiation over social niceties or be controlled by a central, sovereign organization. 
The computer is thus a simulator of the environment. It would hence adjudicate questions posed to it and provide its decisions. No machine can solve questions not posed to it. Any machine can answer questions that are posed most properly. It decides on questions based on previous agreements, which form its axioms. It hence decides on logical formulae based on tautological premises. Each decision however involves evaluating truths based on conditions. These conditions arise from having to evaluate particular environmental signals conveyed as numbers or signals, interpreted by individuals so that it might be referred against a standard lexicological tree (such as code tables) to take the program forward on a given path, till it halts with an accept or reject. 
Humans interpret events from the environment in according to the user interfaces. The program halts till the input is provided.  Where sensors are used to collect data it might loop. There are combinatorially many paths making the system difficult of proof but easy to verify. It could hence be said of the system that it might not even be deterministic, since reproducing the pathway in complex algorithms in production may not be possible. 
Hence, the system is reductive, but non provable. It is consistent to a good extent and hence transactions could happen in a human world so as to serve as a stabilizing consistency checker of human actions. Where human actions are not consistent, or a proposition for an 'action' is made, the sysetem either allows or denies it. 
Hence, it is important to understand that the system plays the role of a permitter/denier of actions, rather than solving intellectual questions.

2) Post Turing Machines.
a) Computers that simulate markets
It is possible to make a network of computers united over a common lexicon and discrete code tables could be programmed with regard to local truths which are expected to be determined adversarially over a network. This is a simple extension of the Turing model. The machine becomes capable of emergent effects such as 'manufacturing consent'. In this case the machine simulates a planar graph, wherein it simulates a rational authority in the system or multiple systems connecting over a network.
There might be indefinite waits on service calls. On the other hand, if the graph is to be made non planar, in the sense, that a certain truth at an earlier point in time, 'arcs' across and influences a later state and likewise where paralell pathways have connections in between them, then the signal processed of the environment, simulates the user events as well. That is to say, if a closed loop simulation is planned, it would require mimicking a free willed agents actions as well. These actions could be simulated by two elements, one is a random generator and secondly a normalizer. 
That is to say, it is more like a MCMC algorithm, where random moves are generated (say a bid is made) and based on a statistical distribution model (by plotting the accepted bids, the system might construct it in memory). Further reejections and acceptance might be made based on such normal distribution. This makes the agent a reflective agent. It tries to reap a surplus by hiding its internal state and pushing towards greater optimization within its context. A cooperative strategy might emerge from this game, leading to a situation where a set of systems might cooperate without the need for common code tables (discrete parameters which identify thresholds of continuous values). However a lexicon is required nevertheless.
Thus intelligent agents might be an extension of a traditional computer with stochastic features that could normalize on the outcomes. But it is also required that such agents could not be fully automated, because of the need for a prior. An uninformative prior or a blind search might not produce an effective system. Hence each service might have its programmers, who program certain outcomes expected wherefrom evidence is observed and a posterior is constructed, by MCMC like methods in one go. Hence, the supplication of intents are important for a computer that simulates markets.

b) Computers that simulate a deliberative agent.
Where a non planar graph model is used, it might involve an extended high dimensional reasoning. For instance, it might construct that the outcome of a certain shipment is dependent not only on the immediate causes, but also from distant causes. For instance, political turmoil in a region might dictate outcomes. If one heuristically knows that the system relies on certain knowledge which is drawn from a limited sample, then he is likely to generate events (such as risk factor) that account for this. Hence, an elaborate network might generate pathways that involve noise filteration. Specific decisions might be cliques which are bijective (such as an eligibility for a loan). The pathways that are traversed to get to either of these mutually exclusive cliques are finite and could be derived from noise filtering over specific pathways, using MCMC like local searches. 

B) Normative Intent of Extended Computing
The normative intent of these extended computing is to be able to simulate extended outcomes and make quotes or bids. That is to say, a system that is intelligent, in that it could answer incomplete questions (by supplying a normal context) might be able to make heuristic judgement, which might lift the burden of human actors. Thus these might be recommenders. Recommenders run a simulation, generating stochastically events from possible points of influence of a system and predict the possible pathway to reach a normative unambiguous point. Based on the pathway that is expected to be followed, suitable preparatory actions are recommended with respect to the initial state, either to maximize desired outcomes or minimize adverse outcomes. 
In the long run, there is only a thin line between decisions and recommendations. If complex decisions are simulated on a computer, the system on the whole is expected to self organize quickly over bots that are high frequency, low cost searches for equilibrium. These equilibria are deterministic. In general human agents are biased, they suspect they can do better than average, that is why people cannot accept a normalization by any agency and would rather like to self determine. 
Hence, a sufficiently intelligent system might simulate its handler as well. It might hence sustain a system autonomously, say we would wait for buses without being able to do anything about it. It then becomes a phenomenon, that outwits observation. There are no ways to nudge or govern the system. This results in long run regression of the system or a historical divergence where the system might incur transaction costs to search for efficient solutions adversarially. Both these situations are uncomfortable.

C) Extending Regular Models to extended models
1) A model high dimensional computing architecture
 We explore the idea of being able to design solutions for the regular business house.
- Firstly, we compose immutable entities. In such a case, every entity would have to have  an initial state from where it starts evolving. There are no updates or deletions, but the specific parts of the entity, say address of a customer keeps changing as if  adding nodes to a graph.
- This allows for not only entities, but also transcient items such as orders to go through processes of creation to closure and forever remain in history. 
- These immutable entities correspond to specific intuitive entities, such as documents that go into a file and remain there for ever (unless archived or specifically destroyed). 
- Every action would involve adding something into a registry or reading from the registry. 
- Thus there might be entities evolving and being linked by reference to other entities. Thus, a document of an order might contain reference to a shipper and a delivery address via indices. 
- It may point to the head of the referred evolutionary path, or might refer to a chronologically dictated connection. Relational databases are efficient in that an automatic pointing to the head is built in, by referential constraints. In older document based databases, this might not be the case. The link is a hardlink, or it may even be directly copied insitu instead of holding references. 
- In fast evolving systems, this may present problems, because there is needed of a hidden index with the timestamp which versions the the branch (such as with git). 
- All branches must however refer to an objective narrative, which lists the codes in use, the identities of actors (whose attributes might change) and a lexicon of abbreviations. Assets might be transcient, but real accounts are to be endured. 
- Where the evolving system branches, say a new branch is added to a vendor, then automated 'pulls' of RDBMS can have problems. 
-If we examine the advantages of this approach, we might see that the ouput graph might be used for a variety of analytical actions, where history is important.
- The graph is planar where we work with realtime transaction processing. The branching is decided by immediate 'events' which signify human processed high dimensional signals. Where such events are not to be the core of the state machine design, such as when automated simulation is preferred, the graph would have to become non planar. 
- There might be higher dimensions to decision making, employed while interpreting environmental events by human agents, as a heuristic excercise.

2) The incremental development of systems around evolutionary pathways, avoid the problem of combinatorial explosion and is capable of deciding questions that are high dimensional. 


Part2


1. Computing ought to be viewed about a boundary. This boundary seperates problems that are natural and that which are synthetic. Synthetic problems arise from human processes solving natural problems. Natural problems are the non linearities in nature. Say a process P in nature could be described by a consistent model M, to a certain degree of completeness. The decisions made using M, would nudge P to dynamically evolve in a certain direction, which is almost deterministic, though from the point of view of P, it is a dynamic evolution. The model M would then have a complementary part M' which would interrupt P and reset it. This interruption would result in surpluses. A clear example could be made out of farming, by performing certain rule bound tasks, waiting, harvesting and repeating. 
Thus, this works as a heat pump, which pumps surplus energy from an order creating system which is not intelligent enough to identify its manipulation, such as with crops or farm animals. There might be a long term fall in marginal utilities or a central regression, however.
Synthetic problems arise from the surplus energy generated by the heat pump. This would need dissipative pumps which would have to make wasteful movements to dissipate the heat. That it is to say, there gets developed complex pathways to negotiate simple dilemmas with respect to reconcillations of liberty and equality. That is to say, how does a person seek to do what is best for him and at the same time expect to compromize with possible conflict with other persons wishes. 
The numerous constructs of organization and politics involve rituals which dissipate energy. These rituals might be a democractic election every five years. The different interest groups are abstracted with an incomplete model and studied in their dynamics, till at decisive point the direction becomes apparant, such as a leftist or rightist swing and appropriate reconstitution of the government takes place. This is an oscillatory movement much like in agriculture. 

2. The role of computing in this setting is the development of an incomplete model which would guide the dynamics to specific targets ready for a second discrete intervention. This happens both in the case of synthetic as well as natural problems. Computing simulates natural problems in order to produce this effect and it could be seen as a continuation of experimental techniques. Likewise in synthetic problems, one might see that there had always been techniques that defined rituals that allowed for ritualistic wasteful movements. Computing by defining less consistent but more complete systems than before allowed for these rituals to happen over higher frequencies, because the dynamics could be computed in narrower time ranges and could be quickly harvested. 
The computing system is a purely self referential system that is placed amidst the society. The presence of this system allows for any arbitrary expression by any agent to be validated for consistency, that is to say, different ways of putting the point that one is entitled to a greater liberty than the neighbor is simplified and validated by the presence of the computing system. This allows for a lesser need to construct dialectic political parties and submission to an internal control system that reductively models, dynamically tracks and decisively resets the system. 


3. The idea itself however seems to be to minimize the use of these dissipative pumps and build orderliness. In more formal terms, it is the role of science to present moral dilemmas. That is to say, science should try to explain by cold reasoning many of the happenings of the universe, so that  a scarce few is left for being referred for moral reasoning. Hence, in this case, we see that it is the duty of science to appreciate the process of computing as reflecting from a general pursuit of evolution, as seeing the agenda of living systems and intelligence as distinct from free willed agents, who would then be presented with a moral dilemma at the end of the scientific apprisal. To get back on the scientific appraisal, every species seeks to attain the highest energy steady state possible. The mosquito develops as much intelligence as would allow it to avoid the most wily swatters, by appropriate reaction. The mosquito had at all times wished to seal the need for dissipation of free energy or the need to explore new forms by stabilizing at a high energy level. But the complex graph of mosquitoes caused several paralell branches at equal levels of sophistication and even some low level creatures being able to draw arcs to defect and continue the evolutionary path, allowing them to stabilize at not the highest possible energy level, but below it.

4. The Ultimate human question might be to find a way where no experimentation might be required. That is to say, all questions had been answered and no further reaping of free energy is required. If the ultimate aim of a species to solve problems in nature, then its pinnacle would be to solve it theoretically and experiment no further. But if reality is not static, rather it carries wave like oscillations by an actor that woudl eventually oscillate through everything. But we think reality is static at this point. Humans might hence seek to attain high energy stability conditions. 

5. A higher integration of this sort happened in multicellularity. But multicellularity is accompanied by a macabre reset, where the grand castle of billions of occupants is brought down by a timed device, only to redraw it completely by reproduction. These billion occupants do have their individual consciousness, but it had resulted in a higher collective consciousness in the CNS. Similar half complete experiments can be seen in eusocial creatures, where a hive mind is existant. But even here, the entire hive collapses at some point and one queen leaves (much like a single gamete selected from the billions of somatic cells going on to restart multicellularity). The redrawl effectively nullifies history and allows the system to add pure stochasticity to its form. The resulting experiment on the environment is hence pure random search, which leads to a redrawn creature in the new environment. The tragedy of life is the need for a reset. All consolidation of intelligence, would eventually lead to the ability for a single clean reset, with an encapsulation in a highly compressed format the whole of the grand enterprise. In simple creatures where such compressed transportation is trivially possible by spores or where cells themselves could be replicated in the vicinity as with algae, there is no grand construction and collapses. However, the collapses themsleves align to the possiblity of a natural collapse due to crowding, invasions by microbes or by simple accidental uncontained local failures.
In another incomplete consolidation, as with niches, say we see that a group of creatures such as bovines, predators and their group form the savannah niche. They explicitly try to increase the range of savannahs by bringing down trees at the edge of the forest, by elephants doing so. They seem to be working as a team with a collective consciousness. There is a recession of consciousness of the individual creatures as they play to their instincts. 
A similar projection might be made of the use of computing on synthetic problems. As natural problems get solved by simulation methods, the productivity gains increase and the use of these by incorporating them into solutions leads to the evolution of the system in strange ways. Say a bus service becomes completely self managing by dealing with fuel suppliers, driving itself, connecting with vendors and suppliers and the entire transportaiton in a metro had been automated. People no longer care about the software patches and control systems. Say over ten years, the systems marginal utlity declines and humans would want to intervene. They would likely find that the progression had been much more, the bus company had actually become a law and order partner, became a major supplier of scrap for defense purposes, making the problem of shutting down the system unpredictable. Hence, the system is effectively free of human control. The subsequent developments would lead to a hive mind like integration. 
But as more and more the integration comes closer to fruition, there would become a need for clean resets. Thus, there would likely emerge a single super computer which would eventually shut down the processes and collapse the system, and then start spawning the process again. Just like individual cells inside multicellular organisms left to starve after the collapse, the individuals would become subjected. 

6. Just as science sees this as a repeating course of the 'phenomenon' of life itself, it also observes that there does exist alternate paths to this seeming determinism. There might arise a certain poing when the agents might start becoming attached to wasteful rituals, as in culture, nations and celebrations and shun computational integration. This had infact been an exposition on the 'phenomenaology of integrated living beings with emergent consciousness' . We also might stand corrected with respect to our previous expostion on 'why no singularity'. But the coutercurrent could be observed (perhaps assisted by fermis paradox) that people infact try to maintain stasis by dissipating energy and without attempting a clean reset and integration pathway. They might in fact attempt to replicate a phenomenon within life itself, attempting terraforming and universal spread without necessarily working vertically. They might leverage on their curiosity and love for liberty and experimentation and value it more than the need for vertical consolidation in singularity. They might prefer variety and deferring away to different directions, hoping for a deferred unity on homecoming, rather than staying put and building elaborate bunkers. But that is the moral dilemma we promised.

Part 3 - Leisure and Defragmentation
It is witnessed that activism arises from some form of outrage. The outrage arises from a perception of the world to have been flawed and below in the standard of constitution of human societies. Hence, most outrages are directed at units of human organizations, which suffer from complexes which are strange and repulsive for the activist. He should hence be moved to avenge this unit of human organization. It is a reflection of how the unit, be it a family as with the Arab joint families, communities or entire countries that motivates him. It is the destruction of the tendency for humans to group together as a natural force and attempt to build a higher level consciousness and substitute it with a grouping over a political consciousness, with the elements of planning, control and above all leisure. 
That is to say, the constant pressure to work as a group while retaining conscious control over the dynamics of the group is the priority of the activist. For this end, he pursues two important actions. Firstly the development of a knowledge of highly consistent forms and secondly on the provisioning of the boundaries of completeness of the said knowledge so that freedom exists in the group to do non impactful activities. Thus, there is required of leisure to be provided for and insulated from made gainful. This repeated reinforcement of the need for consistent platforms to underlie pursuit of reward based actions and its seperation from the society at large is an important way to prevent evolution from inventing a human cohesiveness that is like the tower of Babel experiment. (cite the soft power complex).
The preservation of human agency or liberty to pursue independent spiritual romantic quests while being embedded in a group, capable of being made sense of in political terms presents an ideal which requires the selection of dedicated contributors to this end. Hence activism is a recruitment of people who can do things that does not add to their survivalist needs proportionately. There is a need for a leisure class, unmotivated by hedonism, but rather moved by stoic concerns to create a society with less of unbounded dynamics and more of well appointed dynamic space capable of political interpretations. 
That is to say, a constant flux and dynamic of economic innovation, cross border capital flow, monetary synthetic manipulations, market relationships coupling itself to gender relationships, household generational relationships, religion and institutions of tradition and customs, public spaces and power relationships cause exceptionally complex dynamics, such as uprooting from communities, emigration of family members, traditional roles being interpreted as economic oppression and their challenge through economic market egalitarianism. This we might summarize as fragmentation. 
This fragmentation is caused by a need to pursue greater orderliness than that is local and organized recursively (d-seperated). The more dynamic and shifting the relationships, the greater their need for being tracked by symbolic and quantative pointers. This could then be reintegrated to produce model organizations. This might be in the interest of lower transaction cost to decisions that concern larger wholes, at the cost of local autonomy. In the long run, this produces an integration into a newer consciousness for which there is no pointers from human actors.
The fragmentation is a subject of distress, which however is not quantifiable due to its difficulty of being expressed. People do not become less productive becuase of fragmentation, they suffer nevertheless in silence. The move of the activist is hence to stop the fragmentation by limiting and concretizing notions of modernity so as to allow for a leisurely and voluntary take on the problem, dispersing the forces of ruthless intractable integration, powered by evolution and replacing it with a pluralistic force, which refuses integration and dissipates the excess heat generated by engineering prowess either to solve problems delinked from utility or in the practice of arts.

Hence, activism always concerns itself with acting as  a group of leisure pursuers who identify a specific community or an abstract community to induce changes. These changes might be policy regulation/control or as provisioning of common resources to produce buildings, spaces and free instructions to all. Hence, these actions might be prescriptive, welfare or educative. The motive is redemptive and the medium is formal publication with openness to crticism.
This might be seen as morality in action. Morality concerns with not just principles for stability, but also those for preservation of innate human consciousness. Amorality as in postmodernity might be an antidote to this. Moral sentiments purport people to seperate actions from circumstances
pyschological impact:
In psychological terms, we might percieve that fragmentation, disorients the conginitive faculties into construction of network models of the world which are unstable, leading to volatility resulting from small changes. This instability results in synthetic models where the edges are discredited (where paranaoid delusions might mark cognition) or where nodes are discounted (where a pervasive fatalistic model might influence mood). The resulting adjustment might be permanent and influence command signals to alter it, thereby blocking off further correction by external faculties, or it might be continually oscillating and volatile perception leading to affective volatility and cognitive distortions.




