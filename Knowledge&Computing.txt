An analysis of Computing in the Knowledge theory of Development
==============
Nature of knowledge: Progress might be described as the continuous revelation of the real knowledge. Knowledge is real in that it is singular and exists apart from human imagination or endeavour to appreciate and understand it. This positivist and realist theory of knowledge is what we would consider as the basis of our argument. Thus, scientific systems specify the rules that form the constraints of a system. These rules are coherent with mathematics, such as the electriomagnetic fields being in coherence with vector fields. The rules arising from theories, which map to mathematical theories with logical consistency define only the constraints of the systems, but do not highlight phenomena. Phenomena could be discovered by observation (unbiased) or controlled experimentation. The phenomena would have to satisfy the constraints of theory, which allows for their verification. In humanities we might map phenomena to human actions constrained by legitimacy and legality. The axiom of choice makes it necessary that actions are observable and verifiable but not predictable. Knowledge or theory is hence derived and extended from objective and real things. In the case of humanities as well, soft sciences like economics rely on sound theories in the nature of systems theory, theory of agency and soverignity, the social contract arisign therefrom and so on. Medical sciences lie somewhere in the middle. Hence, despite having noisy observability and reflexive corruption,soft sciences might still have some sound theory. In fact the presence of legal theories is what allows for objective determination of cases.
Human pursuit of perfection or human progress involves an ever increasing access to the real fabric of the cosmos, unperturbed by noise. This allows to locate phenomena in different parts of the real world and humans can carryout projects in the nature of engineering such as bringing together wood and river to produce a primitive dam. This engineering prowess is constrained by rational rules but yet are phenomenological. 
It is possible for people to work with self referential systems, such as mythological tribes. These self referential systems have different notions of reality in a subjective sense. Human progress, particularly in the project of modernity involves a steady adaptation of knowledge that is objective and real, rather than plural and mythological. It had helped in cooperation at a great degree and great engineering inventions in its course.
Nature of Problem solving: Human progress could be defined as a steadily increasing ability to solve problems. Human problem solving involves three phases -> solution, accomodation and consolation. Solution is only part of the approach. Once a solution is provided based on the nature of the 'real' world, that is rationalized and mathematically consistent way, the problem becomes owned by all and hence it is dealt with by accomodation in the social level, by social support and encoded into more complex legislations to protect the underprivilaged. The purely random nature of the problem once established by a consistent arbiter allows for consolation by philosophical and spiritual reflection. It is also a process of seeking completion through romantic contemplation. Hence, a utilitarian frame of progress could be seen to embody a continuous extension of completeness of rational solutions while retaining consistency so as to allow for accomodation and consolation.
From a development perspective, if the solution part involves the use of authority without consistency in a rational framework, then the alternate course is one of vengeance. Knowledge is the ability to evaluate a material development from a consistent rendering of reality in symbols and graphs. It allows for liberation to accomodate and console over uncertainty in nature or to move with vengeance, where the phenomenon of authority is clearly anamolous from the framework of reality.
Computational Knowlege: From the point of view of knowledge, computation relies on pluralistic, self referential, partial recursive constructs and hence might appear as regressive. Computation involves looking for events, which are phenomenological and mapping them onward to further events emitted by the system. The presence of while loops in the fsm model, allows for event gathering and emission. Hence computing is as much about event signalling as data processing. Hence communication and information technology could not be seperated. A computational entity much like an accounting entity is a router of events without specific preferences. In more ancient tribal settings subjective narratives involved events which propogated to create history at every increasing dynamic interactions. But in computing subjectivity, the individual is fragmented and the subjective units exist in equilibrium with others. Programming to events, involve human heuristic actors, who map an incoming event to an outgoing event with a bias towards some optimization subject to the constraints imposed by the partial recursive computing system. Hence, people optimize by programming to certain 'objective' functions subject to the constraints. They give rise to events that are phenomenological rather than knowledge based.
Development based on phenomenological event aggregation and reaction, in a closed loop game, without reference to an objective reality allows for normalization about mean positions. There are involved random experimentaions in shifting away from such means as defection (as with cellular defection) that brings forth rewards at times. Hence, rules are sometimes broken and the resultant advantage leads to evolution of the system as a whole. 
Positioning of ML: The question is whether there exist a 'natural' mapping between events. This might be achieved by looking at the point that eventhough phenomena are free willed, they still could be traced from past events or history. Thus, one might predict that a fast driver would generate an accident event, rather than a slow one. Thus the choice of speed is an event that evolves in time to an event of accident with higher probability. Thus, probability is the study of events which are apparantly random, yet have some motivational reasoning, such as copying or reading from precursors etc. It detects non linearity in the generation of events, normally of tendencies in certain frequency bands that vary in density. They also relate probability distributions where non linearities due to latent factors emerge. The presence of bald tyres and fast drivers might indicate a latent variable which is a conscious disregard for road safety and hence escalates the risk of accident non linearly. This kind of uncovering of 'intentions' allows for mapping between some events and others. It might help in predicting risks of accidents say and make game like reward moves. In case if the ML system attempts to plug into the event field and attempts to normalize and build the non linear statistical joint distribution of the whole, it had infact generated knowledge. This knowledge suffers from two problems
a) Firstly, the notion of reward is instrinsic to it. It needs an objective function to optimize. Hence, it attempts to minimize accidents by putting a high risk score on specific combinations of events. By reward based signals it does not indicate a global standard but only a relative standard. Likewise where a single individual makes a 'causal' action, virtuously by training the drivers in safety, he is not recognized. Hence virtue and vice have no meaning because, to 'cause' has no meaning in statistical inference. This is partially remedied where manual programming is used, because certain special causal events might be programmed in explicitly. 
b) Human progress involves understanding intentional actions as virtuous or vicious, which requires an 'attachment to reality' or sanity. The whole idea of sanity might be cognitively interpreted as referrence to reality, in an objective sense. This reality involves hence being able to arrive at objectively good and bad actions, through such things as theory of mind and empathy. Hence we might say, computational networks (even when programmed as with credit rating applications) drifts towards amorality. This reliance on normal signals and not explicit disturbing causation and intention and its framing on objective concepts of morality leads to a setting where the solution part of the society becomes distant and intractable.
Sanity Crisis : An intractable controller (as against a rationally tractable or rationally unfair one) leads to a situation where humans would have to question their sanity. This might propogate as paranoid or delusionary states. But delusionary states themselves is not absolute and unprovable as with Kantian Idealism. Hence, the development of computing based social setting might infact allow for rational examination but with higher perturbations. The behaviour of phenomenological populations lead to a necessary pressure to follow least energy pathways and hence convergence of local narratives to natural objective realities. Hence, ML systems converge again towards greater knowledge exposition however encoded and interpreted at a different level. Humans might loose their access or touch with reality. It might however filter through the intervening layer of actors dimly.
The evolution of games: Games evolve when orthogonal participants who attempt to optimize locally without regard to global rules, develop some rules locally. The emergence of games indicates the affinity of participants to objective truths. Once a game is set in motion, it also happens that the participants mostly cooperate rather than defect. Cooperation is a strategy of voluntary suboptimal trajectory in common interest. Hence, an awareness of the wholeness, further leads to construction of new rules. Likewise a recursive definition might arrive where rules become defined at ever higher levels. It might also require that the rules be as close as possible to real and natural rules so that the game could be scaled further. Thus, the modernist revolution might be in the making of a repetition with pervasive computing. If emergent sentience is possible, the game participants at higher levels would be making reasoned (atleast strategically tenable) moves. These participants might be higher level games as well. That is to say rule bound entities might emergently make game moves strategically thereby rising truths. 
That is to say, modernity was for individual endeavour to access reality, so that the individual is liberated. It involved a reduction of reward based games on decisions concerning nature. It allowed for accomodation and consolation to be handled at group levels that coexist with an objective problem solving engine. The realignment of individual priorities to work with local narratives leads to a dark age, where wars happen for no known reason for the individual. Even if one is to rise heroically, the game between empires rage due to different narratives. The automatons in such a situation might dominate attainment of agency, make random moves to search for further rules to cooperate. It might be a transhumanist transition, if the ultimate tendency to access reality persists, or if accessing reality is an anthropic affair, the emergent bots would plunge back civilization into the dark ages. Humans might go extinct or sustain a resistance.
