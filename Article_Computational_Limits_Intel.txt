Computational limits of intelligence
A theory of computing as a cyclic truth deducer (and not a machine)
==
1. Science is a way to establish necessary and sufficient conditions for phenomena. Phenomena are crude observations, which are deemed to exist by virtue of their observability. The necessary conditions for the phenomena is established by a logical necessity following a certain principle. A good example might be that there existing a postulation of positivity and negativity and an empirical confirmaiton of electron's existence, would naturally cause the scientist to predict the existence of positively charged particles in the atom. The presence of the electron constitutes the necessary condition for the existence of the positively charged particle. But it might not be sufficient, say what if all the atoms are negatively charged and the balancing charge exists somewhere centrally at a distance. Sufficient conditions are established by empirical verification. 
2. Empirical verification is essentially statistical. Crude observation of phenomena which triggered the inquiry might consist of confounding elements, due to the disturbances in the environment and the intents of the observer. In an experimental setting these are filtered out, so that verification could happen. But absolute noise free environments do not exist and hence all observations are statistical to a good degree. In medicine say a 95pc confidence interval is sufficient to establish the scientific explanation of the phenomenon (a hypothesis seen as H0).
3. Science relies on its six conservation laws and the empirical law of non spontaneity of order creation (law of entropy) to establish necessary conditions, which might itself be expressed mathematically or geometrically. Thus, the discovery of electrons do not supposse that orderliness is established spontaneously by the atom seeking some other positively charged particle in an intentional manner.
4. Accordingly order creation ought to arise from machines. A machine is essentially a heat pump if it creates order. It simply removes the heat (or homogeneous randomness) from an enclosed space onto a heat sink (say the outer space). The act of creation of order by observing the heat (say trapped in the atmosphere) is designed in the machine by a process of mapping discrete events (say the photelectric threshold) to other discrete events (say the critical energy for ATP bond creation). The machine consist of switches and gears which moves and meshes with other gears to create this effect, in a simplistic mechanistic analogy.
5. Complex machines involve paralellism, in that there are multiple components which do not strictly collaborate, but compete or rather orthogonal and coexist on such pardigm. Paralellism would hence involve a multiplicity of systems which optimize locally. The second aspect of complexity is latency, where actions get absorbed into some local sink, which on reaching critical threshold might explosively propogate these actions. From a view point, this latency is akin to stochasticity, in the sense that certain actors (rather than components, since now there is local optimization due to paralellism) hold their appropriate actions till opportune (subjectively) moments. Hence, we might say stochasticity and paralellism designed into machines make them complex heat pumps.
6. Complex machines generate phenomena such as biological systems, which could not be traced to necessary conditions, but only statistically mapped to certain activity in different parts of the system. That is to say they might be modelled as event driven systems where critical events might set the course for certain flows where certain phenomena are anticipated (and not others, we had earlier postulated that phenomena are deemed to exist if we observe them). Hence, divergent outcomes arise in a given model of a system when mapped to the 'real' state of the system (which is unobservable, since the system does not have a central state). We only get to read portions or angles of the system from our model. 
7. As against classical mechanical modelling, these models are state machine based, where for a given cycle certain pathways are traced. The machine or model are hence cyclically convergent on essential truths about the complex system. We presume the complex system is in fact centrally orchestrated and has stochsaticity and paralellism as instrumental features only. Thus, computers look for emergent order in a system, by allowing different sampling from the system by different users over cycles. Likewise the FSM architecture also involves the temporal extension of decisions, in its 'while' loop. It also allows collaboration not only of human agents, but over a network of systems. Hence whether it involves multiple humans or other automata systems, it might be termed as a network.
9. The objective of science is to discover the truth with respect to the originality of a phenomena (rather than it being an effect). The objective of computational networks is to be able to approximate the central intelligence of a system. The utilitarian argument of classical mechanics is to be able to model natural machines, so that these can be emulated in more perfect engineering endeavours (say emulating plant hormones with nitrogenous fertilizers). 
10. The question as to whether utiity is the end of science, becomes a philosophical quesiton, since we are examining the phenomenon of doing science itself. Any phenomenon to which we are party to, triggered by us (humans) and appreciated by us, arise from motivations that transcend immediate concerns of survival and being 'driven' by evolutionary forces. This transcedental drivers of actions are the subject of philosophy. Philosophy is hence beyond empirical observation, becomes it does not seperate the observer from the object, but it is rather reflective. It does not presuppose absolute soverignity of the self, but rather predicts that some essential objective drivers ought to exist that could explain our actions. Such rules might be beyond rational deduction from higher truths as well. All that might be done is to speculate.
11. Speculation might be legitimate, since it might uncover the objective core of a seemingly random crude observation. A speculation is not falehood, only an unbased truth (for which no science - or necessary and sufficient conditions exist). It had been discussed in Bertrand Russels Kettle paradox. Say the various explanations of the Fermi paradox is highly speculative, but given the complex and historical nature of interactions between life and the environment, one might find a great variety of possible speculative pathways. Some of these speculations might be hypothesized and worked upon scientifically.
12. Speculative truths which are not submitted for scientific queries are generally attributed with untruthful motives. But science is not an absolute truth as it might be speculated philosophically. Thus philosophy is disruptive of the normal conduct of affairs of humanity. But modern science originated from philosphical speculation of Galileo and Descartes. The only way speculation can be avoided to undo and backtrack is by the narrative of progress. 
13. There does exist systems which are highly intractable to science and thus making the profitability of engineering innovations in those fields low. These might be economic, biological or political. The central tendencies (the core of central intelligence we talked about) for specific problems in these domains might reveal a mechanism where certain events are mapped to certain other events by design and these designs might be refined and emulated in engineering. This is the grand project of computer networks, where events are mapped by local optimizers (by encoding to partial recursive aximoatic systems) and let to interact with stochasticity and paralellism. The emergent central tendencies are picked up evolutionarily where certain pathways are strengthened by feedback. This leads to an emergent direction of progression of orderliness.
14. The fundamental program of civilization (seen as a network) is however a more plausible query of philosophy. If it had been encoded statically signifies the presence of an apriori truth system. An imperative to embrace the steady gathering of central definitive rules (with additive noise) signifies a definite preference or program. The search of philosophy is to see if there is a contradiction built into this program, as when the program becomes self limiting, such that the drive could be sabotaged by the driven and perhaps reversed. This speculates the critical point where the universe does not allow further progression of order gathering and pushes it back by philosophical questions over the model. 
15. Computer networks allow for additive truths, which would hence act as filter for noisiness and reveal central logic to a system, particularly systems involving humans (as a complex system). This is intended to allow for its refinement and hence progression of orderliness, such as terraforming other planets. Philosophy questions this apriori truth of progress. If there be a materialist program to seek out progress, then is there a critical point when the frustation with meaning (on the ultimate falliability of the material progress given infinity exists) might lead to stopping vain attempts. A stage of philosophical development might hence consign human progress as instrumental to sustaining (given intrustions exist from less intelligent creatures and the universe itself) a campaign of meditation and doing zero sum things or more meaningfully to reflect and contemplate glory.
16. Material progression is still a pertinent and immediate concern of civilization and hence philosophical ruminations of its demotion to an instrumental status might not sell in many parts of the world. Thus philosophy is an absolutist or idealist view of the world, as oppossed to the normative view.
17. In this specific idealist view of computer networks, we might see that computers attempt to involve the observer with the observed in producing a world view, normally a subject of philosophy. This worldview emerges as specific behaviour rather than text encoded truth statements. This view has been discussed in Cybernetics. In evolutionary progression, the subject and object interact to produce persistent behaviour that is no longer dynamically explained by the actors, but are fixed onto to the interaction context.
18. This persistence of behaviour is what defines the uinversal truth to a system, as rules to a game. We say that emergent behaviour beyond critical thresholds might encode itself as definite rules (such as eusocial heirarchies). These rules are intractable to the subjects due to their being encoded with greater degree of noise. Noise might itself be subjective to the observer, a lot more randomness exists to an unintelligent observer such as a dog, than a human. Both possess fundamentally similar contours of reasoning and derivation of meaning, but humans differ in the ability to reason more densely, such as to be able to hold a large memory.
19.Lets take the example of sun rise and sun set being a phenomenon of interest being observed by a human and a dog. The dog sees it as a complex system due to the clouds dimming the sunshine. A long memory might help smoothing the noise to the signal. Paralellism arises from not being able to serialize a situation in high frequency. This in the case of cloud movement would involve serailizing everything even a butterfly's flutter (and farther on, since earth is an open system). If paralellism is essential, then analyzing a complex system is independent of processing power, NP Hard would be NP Hard irrespective of the observer. Minor improvements might be done, but statistical noise would be absolute. However, if one is able to observe a system in isolation, by filtering noise using adequate memory of stable states and oscillatory noise, or one is able to observe a system by physically isolating it by adequate force one might be able isolate components, but the components themselves produce interesting phenomena only holistically. Thus we might say complexity is real, arising from systems interacting with one another, isolating and studying them would not help. Likewise the exponential wall of computation, forbids any increase in intelligence to only make a nominal dent in the construction of meaning of complex phenomena. That would question superintelligence possibility as well (holding on to the serialized definiton of intelligence).
20. That is probably the reason why a lot of knowledge we have is self referential. It simply simulates the universe in terms of known entities and the emergent entities would make better sense in the simulation than in the real world. Hence monsoons are simulated as market prices, compensated by finance hedging and so order is produced from a complex interaction, by additively cancelling out linear actions. The net meaning of the system is attempted to be read. The meaning of monsoon is a certain confidence level of rain outcome, which could be buffered from a reserve to produce a well behaved simulation. Hence, buffered adaptation reduces complexity from a system. It is in effect enclosing the system in a container which is controllable.
21. Buffers are automatic noise observers (even mechanical springs are such). Question is whether a system could be stabilized from within rather than being enclosed. If a buffer is placed within the system as an observer of signals and operating on cancelling them out, such as a good distribution mapping intelligent system ( an exchange say), we might be able to stabilize from within. Computing solves primarily distribution problems by allowing communication while at the same time being only locally optimal. These signals are responded or mapped by the network to stabilize the system. Likewise computing also allows via sensors to collate signals say from buoys of multitude of data, which could be sifted to seperate noise (which could be cancelled out as local disturbances) to long term trends, thereby stabilizing or buffering the system from within. There is no difference between whether the system is placed in a human participating system or a biological or weather system of complex nature. Stability conditions help interpret complex systems.
22. The question is drift. That is to say, once stable conditions are discovered and a system stands effectively buffered from within, an observer at the boundary of the system would be able to observe macroscopic noise filtered changes, which he starts emulating. A spring might resonate micro events. We might observe that noise cancelling by additivity of computing produces a smoothening effect. As long as the whitenoise is seperated, the fundamental signals makes itself apparant to the external observer yielding to control. However, say a certain human population is being observed, if movements of average urbanite is discounted as white noise and the presidential address allows for predicting certain pathway of events, thereby allowing preparing a control strategy, a small statistical rounding off can produce dramatic result. Say an average man pulls the gun on the president, perhaps by impulse. The whole system of control collapses and the system is perturbed dramatically from its stable course. The false sense of stability is only a deferred collapse.
23. The stability obtained by filtering white noise is hence falliable in systems with high heterogenous structure (such as critical points of collapse internally such as the president). The stabilizer hence could not afford simply seperating white noise, but also to internally buffer specific vulnerable points. This would need an understanding of the command structure of the society, where one mans will might by virtue of a heirarchial arrangement be obeyed by others. A more generic analogy of this is the presence of causal relationships, where on one end is a cause and the effect might have no discretion but to necessarily follow the cause, as with the presidential decree. A knowledge of causal relationhips allows understanding heterogenity and all heterogenity is heirarchies of designatable boundaries beyond which causal events dont propogate that well as within the denominated heiarchy. Thus, the presence of causal relationship between raindrop condensation and mountains in the course allows to make a stabilizer or predictor which could incorporate these strong causal patterns to the action. This involves knowledge of heiarchial arrangement of a complex domain, the self contained ordered elements which participate in producing complexity and heterogenous relationships among them.
24. Computing emphasizes seperation of noise to the detriment of derivation of causal relationships. Traditionally knowledge involved qualitative and seperated inquiries into specific associations, involving heirarchies and rules. Such structures permit understanding well established parts of the system but not the whole. In physiology some parts might better understood than some other. To understand a phenomenon, one needs to build a noise filter to buffer and present clear trajectories of health as well as to find specific command relationships so as to be able to approximate them in classical mechanics so that intervention might be provided. The whole however escapes the formalism of mechanism, due to the emergence. Its irreductability hence causes holistic problems be solved inductively from general observations by heuristics. Heuristics is essentially noise filteration, paralell reasoning along possible paths in a Bayesian sense. Humans are good at this apart from causal inference which could be delegated to reductive models.
25. Supplementation by holistic modelling might thus take the burden off heuristics of specialists. Expert systems might hence stand to be supplemented by holistic models. The question is whether definite computational advantages could be wrought by using machines instead of people. We had seen that intelligence hits a wall when dealing with paralellism and a high frequency system is only marginally better than a low frequency system. A low frequency system might be able to passively isolate noise and apply it. 
26. In that high frequency noise filteration is only marginally advantageous it only fuels humans from abstaining from work that they could very well do equally efficiently and substituting by machines. This has economical reasons for substitutions, since long term risks of collapse are hard to come by.
27. It is critical how falliabilities of expert judgements are normatively handled. A falliable decision flows in as frustration, but the good faith with regard to the application of skill by the professional, allows pursuit of consolation either by social accomodation or by spiritual reflection. On the other hand, a computational failure might be interpreted as being biased or having been conjectured not on a professional with reference to an objective framework of ethics (arising from shared heritage as humans etc). Computational holistic models involve work in their construction and hence have value propositions. These value propositions involve preferring work that could be solved quicker rather than slower and the outcome is quantitative rather than qualitative (as with human agents). This had been the trend of additive entities like corporations where a certain degree of quantitative integration had been achieved. Qualitative judgements are generally ignored, requiring people to control them on ethical basis.
28. Hence we present the twin points of any further intelligence in resolving complexity as essentially unattainable and secondly, an algorithm that somehow produces advantages actually defers risks of catastrophes or it actually buffers risks by selecting quantitatively advantageous decisions. Qualitative decisions which could be examined in the system of ethics could not be produced by such machines. Higher frequencies provide short term advantages to seperating white noise, cyclically determining the stable course of the dynamic system, but an ignorance of inherent relationships which have high potential and low inductive representation, requires an understanding of structural principles and initial conditions.
29. Structural principles are discovered not only by an investigation of sufficient conditions, but necessary constraints to the system. This involves deduction from known concepts of generality and specialization, which supposses a priori knowledge, of discreteness of numbers and such principles. A repeated recursive application of these principles to observations produces the effect of seeing them as constituted from parts connected heirarchially and associatively and as enumerations of contained types. On this plane of abstract reasoning a human is able to make sense of the world.
30. The construction of a system of abstract knowledge, could be inductively enriched, but it would need a framework of logical reasoning and mathematical continuity. The objectivism and realism of these entities, might suggest their declaration and capacity of computation on a machine, which we very well do, embodying them in logical structures such as physical machines and logical system of rules. But the computer is different in that it does not represent causal relationhips between entities such as a system of rules, but rather a system that responds to events. A computers course involves a parameter of time to it, making it an automaton but not a classical machine.
31. A computer responds to events, it foresees and alters its internal state till such a state actually involves emitting events. Thus, it maps events to events as an exchange would. It is not a heat pump, it dynamically buffers events to produce stability. It has no notion of heiarchiality or boundaries. It evaluates events (waits for them) and determines if events are there in certain categorical lists and emits events regarding their subjective truth. There is no master plan to their actions (or outgoing signals to actuators). Hence, from the initial state the computer buffers noise but has no means of knowing specific pathways of 'potential' as with causal reasoning. 
32. It might allow development of simulations of human heuristics and adversarial search and thereby some heirarchies might emerge and this preference between good and evil as the dualistic tendencies play in its core reasoning (reward and penalty or truth and untruth either of these dialectic is tenable). Thus, the establishment of some dialectic is required for the development of the computer to reason as humans do. Coupled with inductive noise filtering, they might become crude reasoning machines, but their intelligence might not exceed human ones nor they be highly helpful to human endeavours. Even for this we presume that once a dialectic is present, an universal sense of goodness is available to any open system and could be projected on to the understanding of the world around.

